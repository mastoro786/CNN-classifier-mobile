# âš ï¸ QUICK FIX RESULTS - Still Not Working

**Date:** 2025-11-24  
**Fix Applied:** Padding with 0.0 dB (as suggested by Python team)  
**Result:** âŒ **STILL BIASED TO "NORMAL" CLASS**

---

## ğŸ“Š Test Results

### Test 1: Skizofrenia Audio (SHOULD BE DETECTED AS SKIZOFRENIA)
```
File: pasien_new_x003_long.wav
Duration: 14.22s
Mel spec frames: 609 â†’ truncated to 216 (NO PADDING NEEDED)

Preprocessing stats:
- Min: -125.58 dB
- Max: -0.23 dB
- Mean: -61.29 dB
- Sample values: [-52.79, -46.53, -50.60, -48.08, -48.71, ...]

Model Output:
- Raw sigmoid: 0.001056695356965065
- Predicted: NORMAL (99.9%)  âŒ WRONG!
- Expected: SKIZOFRENIA

Status: âŒ FAILED - Model predicts Normal instead of Skizofrenia
```

### Test 2: Normal Audio
```
File: normal_evelin_01.wav
Duration: 10.20s
Mel spec frames: 436 â†’ truncated to 216 (NO PADDING NEEDED)

Preprocessing stats:
- Min: -117.42 dB
- Max: 0.00 dB
- Mean: -80.46 dB
- Sample values: [-82.31, -72.53, -72.06, -67.26, -67.58, ...]

Model Output:
- Raw sigmoid: 0.000001616611712051963
- Predicted: NORMAL (100.0%)  âœ… CORRECT
- Expected: NORMAL

Status: âœ… PASSED
```

### Test 3: Normal Audio (Corrupted/Silent)
```
File: normal_kris_x001.wav
Duration: 47.05s
Mel spec frames: 2023 â†’ truncated to 216

Preprocessing stats:
- Min: -130.06 dB  âš ï¸ ALL VALUES SAME!
- Max: -130.06 dB
- Mean: -130.06 dB
- Sample values: [-130.06, -130.06, -130.06, ...] (all identical)

Model Output:
- Raw sigmoid: 4.07e-17 (essentially 0)
- Predicted: NORMAL (100.0%)  
- Expected: NORMAL (but audio seems corrupted)

Status: âš ï¸ WARNING - Audio file might be corrupted or mostly silence
```

---

## ğŸ” Analysis

### Issue 1: Padding Fix Didn't Apply
**All test audio files have duration > 9.8s**, which means:
- STFT frames > 216
- **TRUNCATED, not padded**
- **Padding fix (0.0 dB) never executed**

So the padding fix **cannot be tested** with current audio samples!

### Issue 2: Model Still Biased to Normal
Even with preprocessing that looks correct:
- dB range: [-125, 0] âœ…
- Shape: (1, 128, 216, 1) âœ…
- No normalization âœ…
- Proper mel filterbank âœ…

**Model still predicts Normal for Skizofrenia audio!**

This suggests the problem is NOT padding, but something else in preprocessing.

---

## ğŸ¤” Possible Remaining Issues

### 1. Truncation Position
**Flutter**: Takes **FIRST** 216 frames
```dart
spec.sublist(0, targetLen);  // Takes frames 0-215
```

**Python**: Takes first? middle? or **random crop during training**?

If Python training used **random crop** or **center crop**, Flutter taking first 216 frames will be different!

### 2. STFT Center Padding
**Flutter**: Uses `center=False` (no padding at start/end)
```dart
// No padding before/after audio
for (int i = 0; i < numFrames; i++) {
  start = i * hopLength;
  // ...
}
```

**Python librosa default**: `center=True` (pads audio with zeros)
```python
stft = librosa.stft(audio, center=True)  # Default!
```

**Impact**: If Python used `center=True`, frame alignment will be different!

### 3. FFT Bin Count Mismatch
**Flutter**: Uses **1024 bins** (fftSize/2)
```dart
for (int k = 0; k < fftSize ~/ 2; k++) {  // 1024 bins
  power.add(real * real + imag * imag);
}
```

**Python librosa**: Returns **1025 bins** (includes Nyquist frequency)
```python
stft = librosa.stft(...)  # Returns shape (1025, frames)
```

**Impact**: If Python model was trained with 1025 bins, Flutter using 1024 will cause mismatch!

### 4. Mel Filter Normalization
**Flutter**: No normalization (`norm=None` equivalent)
```dart
// Triangular filters without normalization
filter[i][j] = (j - left) / (center - left);
```

**Python**: Might use `norm='slaney'` or `norm=1`
```python
mel_filters = librosa.filters.mel(norm='slaney')  # Normalizes filter area
```

**Impact**: Different mel energy magnitudes â†’ different dB values â†’ wrong predictions!

---

## ğŸ“ Questions for Python Team

### CRITICAL Question 1: STFT Center Padding
```python
# In your training preprocessing, what did you use?
stft = librosa.stft(
    audio,
    n_fft=2048,
    hop_length=512,
    center=?  # True or False? â† VERY IMPORTANT!
)
```

**If `center=True`** (librosa default), Flutter needs major change!

### CRITICAL Question 2: Truncation Method
```python
# If mel spec has > 216 frames, how do you crop it?

# Option A: Take first 216 frames
mel_spec = mel_spec[:216, :]

# Option B: Take center frames
start = (mel_spec.shape[0] - 216) // 2
mel_spec = mel_spec[start:start+216, :]

# Option C: Random crop (during training)
start = random.randint(0, mel_spec.shape[0] - 216)
mel_spec = mel_spec[start:start+216, :]

# Option D: During training, always resize/stretch to 216
# (not crop)
```

**Which one?** This is critical!

### CRITICAL Question 3: FFT Bins
```python
# How many FFT bins does your model use?
stft = librosa.stft(n_fft=2048)
print(stft.shape[0])  # 1025 bins?

# After mel filterbank:
mel_spec = np.dot(mel_filters, stft)
print(mel_filters.shape)  # (128, ????)

# Is it (128, 1024) or (128, 1025)?
```

### CRITICAL Question 4: Mel Filter Normalization
```python
# What normalization did you use?
mel_filters = librosa.filters.mel(
    sr=22050,
    n_fft=2048,
    n_mels=128,
    fmax=8000,
    norm=?  # None, 'slaney', or 1?
)
```

---

## ğŸ§ª Request: Provide EXACT Training Code

Please provide the **EXACT preprocessing function** used during training:

```python
def preprocess_audio_for_training(audio_path):
    """
    EXACT preprocessing used for training.
    Return shape: (128, 216) or (216, 128)?
    """
    # Your exact code here
    audio, sr = librosa.load(audio_path, sr=?)
    
    stft = librosa.stft(
        audio,
        n_fft=?,
        hop_length=?,
        win_length=?,
        window=?,
        center=?,  # â† CRITICAL!
    )
    
    # Power or magnitude?
    power = np.abs(stft) ** 2  # or just np.abs(stft)?
    
    # Mel filterbank
    mel_filters = librosa.filters.mel(
        sr=?,
        n_fft=?,
        n_mels=?,
        fmin=?,
        fmax=?,
        norm=?,  # â† CRITICAL!
    )
    
    mel_spec = np.dot(mel_filters, power)
    
    # Power to dB
    mel_spec_db = librosa.power_to_db(
        mel_spec,
        ref=?,
        amin=?,
        top_db=?
    )
    
    # Truncate/pad
    if mel_spec_db.shape[1] > 216:
        # How do you truncate? â† CRITICAL!
        mel_spec_db = ???
    else:
        # How do you pad? â† Already know: 0.0
        mel_spec_db = np.pad(...)
    
    # Transpose?
    mel_spec_db = mel_spec_db.T  # or not?
    
    # Normalize?
    mel_spec_db = ???  # or keep raw dB?
    
    # Reshape
    input_tensor = mel_spec_db.reshape(1, 128, 216, 1)  # or (1, 216, 128, 1)?
    
    return input_tensor
```

**Send this COMPLETE function** with all parameters filled in!

---

## ğŸ¯ Next Steps

1. âœ… Padding fix applied (0.0 dB) - but **not tested** because no audio needed padding
2. â³ **Waiting for answers** to 4 critical questions
3. â³ **Waiting for exact training preprocessing code**
4. ğŸ”„ Will apply additional fixes based on Python team's response

---

## ğŸ“Š Test Audio Needed

To properly test padding fix, please provide:
- **Short audio** (< 5 seconds) that needs padding
- Both normal and skizofrenia samples
- So we can verify padding with 0.0 dB works correctly

---

## ğŸš¨ Current Status

**Model Accuracy:**
- âœ… Normal class: 100% correct (2/2 valid samples)
- âŒ Skizofrenia class: 0% correct (0/1 sample)
- **Overall: Still biased to Normal class!**

**Conclusion:** Padding fix alone is NOT sufficient. Need to identify other preprocessing mismatches.

---

*Document created: 2025-11-24*  
*Flutter app version: 1.5.0*  
*Quick fix status: INCOMPLETE - waiting for more info from Python team*
